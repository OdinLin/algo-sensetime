/**
  * Created by yuanpingzhou on 10/19/16.
*/
package com.sensetime.ad.algo.ctr

object FMWithSGD {
  import org.apache.spark.SparkContext
  import org.apache.spark.SparkContext._
  import org.apache.spark.SparkConf
  import org.apache.spark.rdd.RDD
  import org.apache.spark.mllib.util.MLUtils
  import org.apache.spark.mllib.regression.LabeledPoint
  import org.apache.spark.mllib.linalg.{Vectors, Vector => SparkV, SparseVector => SparkSV, DenseVector => SparkDV, Matrix => SparkM }
  import breeze.linalg.{Vector => BV, SparseVector => BSV, DenseVector => BDV, DenseMatrix => BDM, _}
  //import breeze.numerics._
  import breeze.numerics.sqrt
  import breeze.numerics.exp
  import breeze.numerics.log
  //import scala.math._
  import util.Random.shuffle

  implicit def toBreeze(v: SparkV) : BV[Double]= {
    /** Convert a spark.mllib.linalg vector into breeze.linalg.vector
      * We use Breeze library for any type of matrix operations because mllib local linear algebra package doesn't have any support for it (in Spark 1.4.1)
      * mllib toBreeze function already exists but is private to mllib scope
      *
      * Automatically choose the representation (dense or sparse) which use the less memory to be store
      */
    val nnz = v.numNonzeros
    if (1.5 * (nnz +1.0) < v.size) {
      new BSV(v.toSparse.indices, v.toSparse.values, v.size)
    } else {
      BV(v.toArray)
    }

  }

  def fm_get_p (X: SparkV, W : BDM[Double]) : Double = {
    val nnz = X.numNonzeros
    var x:BV[Double] = BV(0.0)
    var w:BDM[Double] = BDM(0.0)
    /**	Computes the probability of an instance given a model */

    // convert Spark Vector to Breeze Vector
    if (1.5 * (nnz +1.0) < X.size) {
      val xsp = X.toSparse
      val xind = xsp.indices.toSeq
      x = BV(xsp.values)
      w = W(xind,::).toDenseMatrix
    } else {
      x = X:BV[Double]
      w = W
    }

    val xa = x.toDenseVector.asDenseMatrix
    val VX =  xa * w
    val VX_square = (xa :* xa)  * (w :* w)

    val phi = 0.5*(VX:*VX - VX_square).sum
    return 1/(1 + exp(-phi))

  }

  def predictFM (data : RDD[LabeledPoint], W : BDM[Double]) : RDD[Double] = {
    /** Computes the probabilities given a model for the complete data set */
    return data.map(row => fm_get_p(row.features, W))
  }

  def logloss(y_pred : Array[Double], y_true : Array[Int]) : Double = {
    /* Computes the logloss given the true label and the predictions */
    val losses: BDV[Double] =  BDV(y_true.map(v => v.toDouble) :* (y_pred.map(log(_)))) +
                  BDV(y_true.map(v => (1-v).toDouble) :* (y_pred.map(v => 1-v).map(log(_))))
    return -losses.sum / losses.size
  }

  // cross entropy loss for label 1/0
  //  -ylog(p) - (1 - y)log(1-p)
  def evaluate (data : RDD[LabeledPoint], w : BDM[Double]) : Double = {
    /*  Evaluate a Factorization Machine model on a data set.
    *
    *    Parameters:
    *    data : RDD of LabeledPoints
    *            Evaluation data. Labels should be -1 and 1
    *    w : Breeze dense matrix
    *            FM model, result from trainFM_sgd or trainFM_parallel_sgd
    *   return:
        logl: average logloss
    */
    val y_true_rdd = data.map(lp => if(lp.label == 1){1} else {0})
    val y_true = y_true_rdd.collect()
    val y_pred_rdd = predictFM(data, w)
    val y_pred = y_pred_rdd.collect()

    val ret = logloss(y_pred, y_true)
    //val ret = computeAccuracy(y_pred,y_true)

    ret
  }

  //
  def predict(X: SparkV,W: BDM[Double]): Double = {
    val nnz = X.numNonzeros
    var x:BV[Double] = BV(0.0)
    var w:BDM[Double] = BDM(0.0)

    // convert Spark Vector to Breeze Vector
    if (1.5 * (nnz +1.0) < X.size) {
      val xsp = X.toSparse
      val xind = xsp.indices.toSeq
      x = BV(xsp.values)
      w = W(xind,::).toDenseMatrix
    } else {
      x = X:BV[Double]
      w = W
    }

    val xa = x.toDenseVector.asDenseMatrix
    val VX =  xa * w
    val VX_square = (xa :* xa)  * (w :* w)

    val phi: Double = 0.5*(VX:*VX - VX_square).sum
    //val expnyt = exp(-Y*phi)
    //log(1 + expnyt)
    phi
  }

  def predictFM2(data: RDD[LabeledPoint],w: BDM[Double]): RDD[Double] = {
    data.map(lp => predict(lp.features,w))
  }

  def computeAccuracy(predict: Array[Double],truth: Array[Double]): Double = {
    val tupled = predict.zip(truth)
    val hit = BDV(
      tupled.map(
        pair => {
          println(pair._1,pair._2)
          if (((pair._1.toDouble > 0) && (pair._2.toDouble > 0)) || ((pair._1.toDouble <= 0) && (pair._2.toDouble <= 0)))
            1
          else
            0
        }
      )
    )
    (1.0 * hit.sum)/hit.size
  }

  // exponential loss for label 1/-1
  // log(1 + exp(-yp))
  def evaluate2 (data : RDD[LabeledPoint], w : BDM[Double]) : Double = {

    val predict = predictFM2(data,w)
    val truth = data.map(lp => lp.label)
    //val reg = 0.5 * regPara * (w:*w)
    //val expl = loss.sum + reg.sum
    val accuracy = computeAccuracy(predict.collect,truth.collect)

    accuracy
    //1.0
  }

  def fm_gradient_sgd_trick (X: BV[Double], y: Double , W: BDM[Double], regParam: Double): BDM[Double] = {
    /* 	Computes the gradient for one instance using Rendle FM paper (2010) trick (linear time computation) */
    val nrFeat = X.size
    val xa = X.toDenseVector.asDenseMatrix
    val x_matrix = xa.t * xa
    val VX =  xa * W
    val VX_square = (xa :* xa)  * (W :* W)

    val phi = 0.5*(VX:*VX - VX_square).sum
    val expnyt = exp(-y*phi)
    var i = 0
    while (i < nrFeat) {
      x_matrix.update(i, i, 0.0)
      i += 1
    }

    var result = x_matrix * W :*(-y*expnyt)/(1+expnyt)

    result+= W:*regParam

    return result
  }

  def sgd_subset(train_X : Array[SparkV], train_Y : Array[Double], W : BDM[Double], iter_sgd : Int, alpha : Double, regParam : Double) : BDM[Double] =  {
    /*    Computes stochastic gradient descent for a partition (in memory) */

    val N = train_X.length
    var wsub : BDM[Double] = BDM.zeros(W.rows,W.cols)
    wsub += W
    var G = BDM.ones[Double](W.rows,W.cols)

    for (i <- 1 to iter_sgd) {
      var random_idx_list = shuffle(0 to N-1)
      for (j <- 0 to N-1) {
        val idx = random_idx_list(j)
        val X = train_X(idx)
        val y = train_Y(idx)
        val nnz = X.numNonzeros
        if (1.5 * (nnz +1.0) < X.size) {
          val xsp = X.toSparse
          val xind = xsp.indices.toSeq
          val grads_compress = fm_gradient_sgd_trick(BV(xsp.values), y, wsub(xind,::).toDenseMatrix, regParam)
          G(xind,::) := (G(xind,::).toDenseMatrix + (grads_compress :* grads_compress))
          wsub(xind,::) := wsub(xind,::).toDenseMatrix - (alpha :* (grads_compress :/ (G(xind,::).toDenseMatrix.map(sqrt(_)))))

        } else {
          val grads = fm_gradient_sgd_trick(X, y, wsub, regParam)

          G += grads :* grads
          // alpha * (G.map(sqrt(_)) part is learning rate which variate with gradient
          wsub -= alpha * grads :/ (G.map(sqrt(_)))

        }
      }
    }
    return wsub
  }

  def trainFM_parallel_sgd (sc : SparkContext, data : RDD[LabeledPoint], iterations: Int = 50, iter_sgd : Int =5, alpha : Double =0.01,
                            regParam : Double = 0.0, factorLength : Int = 4, verbose: Boolean =false) : BDM[Double] = {
    /*
    * Train a Factorization Machine model using parallel stochastic gradient descent.
    *
    * Parameters:
    * data : RDD of LabeledPoints
    *    Training data. Labels should be -1 and 1
    *    Features should be Vector from mllib.linalg library
    * iterations : Int
    *    Nr of iterations of parallel SGD. default=50
    * iter_sgd : Int
    *	Nr of iteration of sgd in each partition. default = 5
    * alpha : Double
    *    Learning rate of SGD. default=0.01
    * regParam : Double
    *    Regularization parameter. default=0.01
    * factorLength : Int
    *    Length of the weight vectors of the FMs. default=4
    * verbose: Boolean
    *    Whether to ouptut iteration numbers, time, logloss for train and validation sets
    * returns: W
    *    Breeze dense matrix holding the model weights
    */
    var train = data
    var valid = data
    if (verbose) {
      val Array(tr,te) = data.randomSplit(Array(0.8, 0.2))
      train = tr
      valid = te
    }
    train.cache()
    valid.cache()

    // train.map(...) return RDD[U] type
    // glom convert RDD[U] into RDD[Array[U]]
    val train_X = train.map(xy => xy.features).glom()
    val train_Y = train.map(xy => xy.label).glom()
    val train_XY = train_X.zip(train_Y)
    train_XY.cache()

    // train_XY.first() return tuple(train_X,train_Y)
    // train_XY.first()._1 return train_X , RDD[Array] type
    // train_XY.first()._1(0) return first element of Array
    val nrFeat = train_XY.first()._1(0).size
    // feature factorization
    // generate factorization matrix
    //var W: BDM[Double] = BDM.rand(nrFeat,factorLength)
    val rand = breeze.stats.distributions.Gaussian(0, 0.1)
    var W: BDM[Double] = BDM.rand(nrFeat,factorLength,rand)
    // amplify the matrix
    //W :*= 1/sqrt(sum(W:*W))
    //W :*=  1/sqrt((W:*W).sum)
    //W.map(v => println(v))

    println("train records : %d   valid records : %d ".format(train.count(),valid.count()))
    if (verbose) {
      println("iter   train_logl  valid_logl")
      //println("%d         %.5f   %.5f".format(0, evaluate(train, W), evaluate(valid, W)))
      println("%d          %.5f".format(0,  evaluate2(valid, W)))
    }

    train_XY.repartition(2)
    for (i <- 1 to iterations) {
      val wb = sc.broadcast(W)
      val wsub = train_XY.map(xy => sgd_subset(xy._1, xy._2, wb.value, iter_sgd, alpha, regParam))
      W = wsub.map(w=> w.map(_/2)).reduce(_+_)
      if (verbose) {
        //println("%d         %.5f   %.5f".format(i, evaluate(train, W), evaluate(valid, W)))
        println("%d         %.5f".format(i, evaluate2(valid, W)))
      }
    }
    train_XY.unpersist()
    return W
  }

  def formatData(data: RDD[String]): RDD[LabeledPoint] = {
    val formated: RDD[LabeledPoint] = data.map{
      line =>
        val tokens = line.trim.split(" ",-1)
        val label = tokens(0).toInt
        var features: BDV[Double] = BDV.zeros(123)
        tokens.slice(1,tokens.length).map{
          x =>
            val hit: Int = x.split(":",-1)(0).toInt
            features.update(hit - 1,1.toDouble)
        }
        LabeledPoint(label,Vectors.dense(features.toArray))
    }
    formated
  }

  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster(args(0)).setAppName("FMWithSGD")//.set("spark.executor.memory", "6g")
    val sc = new SparkContext(conf)

    var rawRDD = sc.textFile(args(1))
    println("Total records : " + rawRDD.count)

    val trainRDD = formatData(rawRDD)
    val crossFeatWeight = trainFM_parallel_sgd(sc,trainRDD,10,5,0.00003,0.8,10,true)

    rawRDD = sc.textFile(args(2))
    val testRDD = formatData(rawRDD)
    val loss = evaluate2(testRDD,crossFeatWeight)
    println("loss on test data : %.5f".format(loss))
  }
}
